<!DOCTYPE HTML>
<html lang="zh" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>malloc &amp; sysmalloc</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../../../favicon.svg">
        <link rel="shortcut icon" href="../../../favicon.png">
        <link rel="stylesheet" href="../../../css/variables.css">
        <link rel="stylesheet" href="../../../css/general.css">
        <link rel="stylesheet" href="../../../css/chrome.css">
        <link rel="stylesheet" href="../../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../../highlight.css">
        <link rel="stylesheet" href="../../../tomorrow-night.css">
        <link rel="stylesheet" href="../../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../../../toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../../../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="../../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="malloc--sysmalloc"><a class="header" href="#malloc--sysmalloc">malloc &amp; sysmalloc</a></h1>
<p>{% hint style="success" %}
学习与实践 AWS 黑客技术：<img src="/.gitbook/assets/arte.png" alt="" data-size="line"><a href="https://training.hacktricks.xyz/courses/arte"><strong>HackTricks 培训 AWS 红队专家 (ARTE)</strong></a><img src="/.gitbook/assets/arte.png" alt="" data-size="line"><br />
学习与实践 GCP 黑客技术：<img src="/.gitbook/assets/grte.png" alt="" data-size="line"><a href="https://training.hacktricks.xyz/courses/grte"><strong>HackTricks 培训 GCP 红队专家 (GRTE)</strong><img src="/.gitbook/assets/grte.png" alt="" data-size="line"></a></p>
<details>
<summary>支持 HackTricks</summary>
<ul>
<li>查看 <a href="https://github.com/sponsors/carlospolop"><strong>订阅计划</strong></a>!</li>
<li><strong>加入</strong> 💬 <a href="https://discord.gg/hRep4RUj7f"><strong>Discord 群组</strong></a> 或 <a href="https://t.me/peass"><strong>Telegram 群组</strong></a> 或 <strong>关注</strong> 我们的 <strong>Twitter</strong> 🐦 <a href="https://twitter.com/hacktricks_live"><strong>@hacktricks_live</strong></a><strong>.</strong></li>
<li><strong>通过向</strong> <a href="https://github.com/carlospolop/hacktricks"><strong>HackTricks</strong></a> 和 <a href="https://github.com/carlospolop/hacktricks-cloud"><strong>HackTricks Cloud</strong></a> GitHub 仓库提交 PR 分享黑客技巧。</li>
</ul>
</details>
{% endhint %}
<h2 id="分配顺序摘要"><a class="header" href="#分配顺序摘要">分配顺序摘要 <a href="#libc_malloc" id="libc_malloc"></a></a></h2>
<p>（此摘要中未解释检查，并且省略了一些案例以简洁）</p>
<ol>
<li><code>__libc_malloc</code> 尝试从 tcache 获取一个块，如果没有，则调用 <code>_int_malloc</code></li>
<li><code>_int_malloc</code> : </li>
<li>尝试生成 arena 如果没有的话</li>
<li>如果有任何正确大小的快速 bin 块，使用它</li>
<li>用其他快速块填充 tcache</li>
<li>如果有任何正确大小的小 bin 块，使用它</li>
<li>用该大小的其他块填充 tcache</li>
<li>如果请求的大小不适用于小 bin，将快速 bin 合并到未排序 bin</li>
<li>检查未排序 bin，使用第一个有足够空间的块</li>
<li>如果找到的块更大，则将其分割以返回一部分，并将剩余部分添加回未排序 bin</li>
<li>如果块的大小与请求的大小相同，则使用它填充 tcache 而不是返回（直到 tcache 满，然后返回下一个）</li>
<li>对于检查的每个较小大小的块，将其放入其各自的小或大 bin</li>
<li>检查请求大小索引中的大 bin</li>
<li>从第一个大于请求大小的块开始查找，如果找到则返回并将剩余部分添加到小 bin</li>
<li>从下一个索引开始检查大 bin 直到结束</li>
<li>从下一个更大的索引检查任何块，将第一个找到的块分割以用于请求的大小，并将剩余部分添加到未排序 bin</li>
<li>如果在之前的 bin 中未找到任何内容，从顶部块获取一个块</li>
<li>如果顶部块不够大，则使用 <code>sysmalloc</code> 扩大它</li>
</ol>
<h2 id="__libc_malloc"><a class="header" href="#__libc_malloc">__libc_malloc <a href="#libc_malloc" id="libc_malloc"></a></a></h2>
<p><code>malloc</code> 函数实际上调用 <code>__libc_malloc</code>。此函数将检查 tcache 以查看是否有任何可用的所需大小的块。如果有，它将使用它；如果没有，它将检查是否为单线程，在这种情况下，它将在主 arena 中调用 <code>_int_malloc</code>，如果不是，它将在线程的 arena 中调用 <code>_int_malloc</code>。</p>
<details>
<summary>__libc_malloc 代码</summary>
```c
// From https://github.com/bminor/glibc/blob/master/malloc/malloc.c
<p>#if IS_IN (libc)
void *
__libc_malloc (size_t bytes)
{
mstate ar_ptr;
void *victim;</p>
<p>_Static_assert (PTRDIFF_MAX &lt;= SIZE_MAX / 2,
"PTRDIFF_MAX is not more than half of SIZE_MAX");</p>
<p>if (!__malloc_initialized)
ptmalloc_init ();
#if USE_TCACHE
/* int_free also calls request2size, be careful to not pad twice.  */
size_t tbytes = checked_request2size (bytes);
if (tbytes == 0)
{
__set_errno (ENOMEM);
return NULL;
}
size_t tc_idx = csize2tidx (tbytes);</p>
<p>MAYBE_INIT_TCACHE ();</p>
<p>DIAG_PUSH_NEEDS_COMMENT;
if (tc_idx &lt; mp_.tcache_bins
&amp;&amp; tcache != NULL
&amp;&amp; tcache-&gt;counts[tc_idx] &gt; 0)
{
victim = tcache_get (tc_idx);
return tag_new_usable (victim);
}
DIAG_POP_NEEDS_COMMENT;
#endif</p>
<p>if (SINGLE_THREAD_P)
{
victim = tag_new_usable (_int_malloc (&amp;main_arena, bytes));
assert (!victim || chunk_is_mmapped (mem2chunk (victim)) ||
&amp;main_arena == arena_for_chunk (mem2chunk (victim)));
return victim;
}</p>
<p>arena_get (ar_ptr, bytes);</p>
<p>victim = _int_malloc (ar_ptr, bytes);
/* Retry with another arena only if we were able to find a usable arena
before.  */
if (!victim &amp;&amp; ar_ptr != NULL)
{
LIBC_PROBE (memory_malloc_retry, 1, bytes);
ar_ptr = arena_get_retry (ar_ptr, bytes);
victim = _int_malloc (ar_ptr, bytes);
}</p>
<p>if (ar_ptr != NULL)
__libc_lock_unlock (ar_ptr-&gt;mutex);</p>
<p>victim = tag_new_usable (victim);</p>
<p>assert (!victim || chunk_is_mmapped (mem2chunk (victim)) ||
ar_ptr == arena_for_chunk (mem2chunk (victim)));
return victim;
}</p>
<pre><code>&lt;/details&gt;

注意它将始终用 `tag_new_usable` 标记返回的指针，来自代码：
```c
void *tag_new_usable (void *ptr)

Allocate a new random color and use it to color the user region of
a chunk; this may include data from the subsequent chunk's header
if tagging is sufficiently fine grained.  Returns PTR suitably
recolored for accessing the memory there.
</code></pre>
<h2 id="_int_malloc"><a class="header" href="#_int_malloc">_int_malloc <a href="#int_malloc" id="int_malloc"></a></a></h2>
<p>这是分配内存的函数，使用其他桶和顶部块。</p>
<ul>
<li>开始</li>
</ul>
<p>它开始定义一些变量并获取请求的内存空间所需的实际大小：</p>
<details>
<summary>_int_malloc 开始</summary>
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L3847
static void *
_int_malloc (mstate av, size_t bytes)
{
INTERNAL_SIZE_T nb;               /* normalized request size */
unsigned int idx;                 /* associated bin index */
mbinptr bin;                      /* associated bin */
<p>mchunkptr victim;                 /* inspected/selected chunk <em>/
INTERNAL_SIZE_T size;             /</em> its size <em>/
int victim_index;                 /</em> its bin index */</p>
<p>mchunkptr remainder;              /* remainder from a split <em>/
unsigned long remainder_size;     /</em> its size */</p>
<p>unsigned int block;               /* bit map traverser <em>/
unsigned int bit;                 /</em> bit map traverser <em>/
unsigned int map;                 /</em> current word of binmap */</p>
<p>mchunkptr fwd;                    /* misc temp for linking <em>/
mchunkptr bck;                    /</em> misc temp for linking */</p>
<p>#if USE_TCACHE
size_t tcache_unsorted_count;	    /* count of unsorted chunks processed */
#endif</p>
<p>/*
Convert request size to internal form by adding SIZE_SZ bytes
overhead plus possibly more to obtain necessary alignment and/or
to obtain a size of at least MINSIZE, the smallest allocatable
size. Also, checked_request2size returns false for request sizes
that are so large that they wrap around zero when padded and
aligned.
*/</p>
<p>nb = checked_request2size (bytes);
if (nb == 0)
{
__set_errno (ENOMEM);
return NULL;
}</p>
<pre><code>&lt;/details&gt;

### Arena

在不太可能的情况下，如果没有可用的 arena，它会使用 `sysmalloc` 从 `mmap` 获取一个块：

&lt;details&gt;

&lt;summary&gt;_int_malloc not arena&lt;/summary&gt;
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L3885C3-L3893C6
/* There are no usable arenas.  Fall back to sysmalloc to get a chunk from
mmap.  */
if (__glibc_unlikely (av == NULL))
{
void *p = sysmalloc (nb, av);
if (p != NULL)
alloc_perturb (p, bytes);
return p;
}
</code></pre>
</details>
<h3 id="快速堆"><a class="header" href="#快速堆">快速堆</a></h3>
<p>如果所需的大小在快速堆的范围内，尝试使用快速堆中的一个块。基本上，根据大小，它会找到有效块应该位于的快速堆索引，如果有，它会返回其中一个。<br />
此外，如果启用了 tcache，它会<strong>用快速堆填充该大小的 tcache 桶</strong>。</p>
<p>在执行这些操作时，会执行一些安全检查：</p>
<ul>
<li>如果块未对齐：<code>malloc(): unaligned fastbin chunk detected 2</code></li>
<li>如果前向块未对齐：<code>malloc(): unaligned fastbin chunk detected</code></li>
<li>如果返回的块的大小因其在快速堆中的索引而不正确：<code>malloc(): memory corruption (fast)</code></li>
<li>如果用于填充 tcache 的任何块未对齐：<code>malloc(): unaligned fastbin chunk detected 3</code></li>
</ul>
<details>
<summary>_int_malloc 快速堆</summary>
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L3895C3-L3967C6
/*
If the size qualifies as a fastbin, first check corresponding bin.
This code is safe to execute even if av is not yet initialized, so we
can try it without checking, which saves some time on this fast path.
*/
<p>#define REMOVE_FB(fb, victim, pp)			<br />
do							<br />
{							<br />
victim = pp;					<br />
if (victim == NULL)				<br />
break;						<br />
pp = REVEAL_PTR (victim-&gt;fd);                                     <br />
if (__glibc_unlikely (pp != NULL &amp;&amp; misaligned_chunk (pp)))       <br />
malloc_printerr ("malloc(): unaligned fastbin chunk detected"); <br />
}							<br />
while ((pp = catomic_compare_and_exchange_val_acq (fb, pp, victim)) <br />
!= victim);					\</p>
<p>if ((unsigned long) (nb) &lt;= (unsigned long) (get_max_fast ()))
{
idx = fastbin_index (nb);
mfastbinptr *fb = &amp;fastbin (av, idx);
mchunkptr pp;
victim = *fb;</p>
<p>if (victim != NULL)
{
if (__glibc_unlikely (misaligned_chunk (victim)))
malloc_printerr ("malloc(): unaligned fastbin chunk detected 2");</p>
<p>if (SINGLE_THREAD_P)
<em>fb = REVEAL_PTR (victim-&gt;fd);
else
REMOVE_FB (fb, pp, victim);
if (__glibc_likely (victim != NULL))
{
size_t victim_idx = fastbin_index (chunksize (victim));
if (__builtin_expect (victim_idx != idx, 0))
malloc_printerr ("malloc(): memory corruption (fast)");
check_remalloced_chunk (av, victim, nb);
#if USE_TCACHE
/</em> While we're here, if we see other chunks of the same size,
stash them in the tcache.  */
size_t tc_idx = csize2tidx (nb);
if (tcache != NULL &amp;&amp; tc_idx &lt; mp_.tcache_bins)
{
mchunkptr tc_victim;</p>
<p>/* While bin not empty and tcache not full, copy chunks.  */
while (tcache-&gt;counts[tc_idx] &lt; mp_.tcache_count
&amp;&amp; (tc_victim = *fb) != NULL)
{
if (__glibc_unlikely (misaligned_chunk (tc_victim)))
malloc_printerr ("malloc(): unaligned fastbin chunk detected 3");
if (SINGLE_THREAD_P)
*fb = REVEAL_PTR (tc_victim-&gt;fd);
else
{
REMOVE_FB (fb, pp, tc_victim);
if (__glibc_unlikely (tc_victim == NULL))
break;
}
tcache_put (tc_victim, tc_idx);
}
}
#endif
void *p = chunk2mem (victim);
alloc_perturb (p, bytes);
return p;
}
}
}</p>
<pre><code>&lt;/details&gt;

### 小型堆

如注释所示，小型堆每个索引只保存一种大小，因此检查是否有有效的块可用非常快速，因此在快速堆之后，会检查小型堆。

第一次检查是确定请求的大小是否可能在小型堆中。在这种情况下，获取对应的小型堆中的**索引**，并查看是否有**任何可用块**。

然后，进行安全检查，检查：

* &amp;#x20;如果 `victim-&gt;bk-&gt;fd = victim`。以确保两个块正确链接。

在这种情况下，块**设置`inuse`位，**双向链表被修复，因此该块从中消失（因为它将被使用），并在需要时设置非主区域位。

最后，**用小型堆中的其他块（如果有）填充请求大小的tcache索引**。 

&lt;details&gt;

&lt;summary&gt;_int_malloc 小型堆&lt;/summary&gt;
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L3895C3-L3967C6

/*
If a small request, check regular bin.  Since these "smallbins"
hold one size each, no searching within bins is necessary.
(For a large request, we need to wait until unsorted chunks are
processed to find best fit. But for small ones, fits are exact
anyway, so we can check now, which is faster.)
*/

if (in_smallbin_range (nb))
{
idx = smallbin_index (nb);
bin = bin_at (av, idx);

if ((victim = last (bin)) != bin)
{
bck = victim-&gt;bk;
if (__glibc_unlikely (bck-&gt;fd != victim))
malloc_printerr ("malloc(): smallbin double linked list corrupted");
set_inuse_bit_at_offset (victim, nb);
bin-&gt;bk = bck;
bck-&gt;fd = bin;

if (av != &amp;main_arena)
set_non_main_arena (victim);
check_malloced_chunk (av, victim, nb);
#if USE_TCACHE
/* While we're here, if we see other chunks of the same size,
stash them in the tcache.  */
size_t tc_idx = csize2tidx (nb);
if (tcache != NULL &amp;&amp; tc_idx &lt; mp_.tcache_bins)
{
mchunkptr tc_victim;

/* While bin not empty and tcache not full, copy chunks over.  */
while (tcache-&gt;counts[tc_idx] &lt; mp_.tcache_count
&amp;&amp; (tc_victim = last (bin)) != bin)
{
if (tc_victim != 0)
{
bck = tc_victim-&gt;bk;
set_inuse_bit_at_offset (tc_victim, nb);
if (av != &amp;main_arena)
set_non_main_arena (tc_victim);
bin-&gt;bk = bck;
bck-&gt;fd = bin;

tcache_put (tc_victim, tc_idx);
}
}
}
#endif
void *p = chunk2mem (victim);
alloc_perturb (p, bytes);
return p;
}
}
</code></pre>
</details>
<h3 id="malloc_consolidate"><a class="header" href="#malloc_consolidate">malloc_consolidate</a></h3>
<p>如果它不是一个小块，那么就是一个大块，在这种情况下调用 <strong><code>malloc_consolidate</code></strong> 以避免内存碎片。</p>
<details>
<summary>malloc_consolidate 调用</summary>
```c
/*
If this is a large request, consolidate fastbins before continuing.
While it might look excessive to kill all fastbins before
even seeing if there is space available, this avoids
fragmentation problems normally associated with fastbins.
Also, in practice, programs tend to have runs of either small or
large requests, but less often mixtures, so consolidation is not
invoked all that often in most programs. And the programs that
it is called frequently in otherwise tend to fragment.
*/
<p>else
{
idx = largebin_index (nb);
if (atomic_load_relaxed (&amp;av-&gt;have_fastchunks))
malloc_consolidate (av);
}</p>
<pre><code>&lt;/details&gt;

malloc consolidate 函数基本上从快速链表中移除块，并将它们放入未排序链表中。在下一个 malloc 之后，这些块将被组织到各自的小/快速链表中。

请注意，如果在移除这些块时，发现它们与未使用的前一个或后一个块相连，它们将被 **解除链接并合并**，然后将最终块放入 **未排序** 链表中。

对于每个快速链表块，执行了一些安全检查：

* 如果块未对齐触发： `malloc_consolidate(): unaligned fastbin chunk detected`
* 如果块的大小与其所在索引应有的大小不同： `malloc_consolidate(): invalid chunk size`
* 如果前一个块未使用，并且前一个块的大小与 `prev_chunk` 指示的大小不同： `corrupted size vs. prev_size in fastbins`

&lt;details&gt;

&lt;summary&gt;malloc_consolidate function&lt;/summary&gt;
```c
// https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L4810C1-L4905C2

static void malloc_consolidate(mstate av)
{
mfastbinptr*    fb;                 /* current fastbin being consolidated */
mfastbinptr*    maxfb;              /* last fastbin (for loop control) */
mchunkptr       p;                  /* current chunk being consolidated */
mchunkptr       nextp;              /* next chunk to consolidate */
mchunkptr       unsorted_bin;       /* bin header */
mchunkptr       first_unsorted;     /* chunk to link to */

/* These have same use as in free() */
mchunkptr       nextchunk;
INTERNAL_SIZE_T size;
INTERNAL_SIZE_T nextsize;
INTERNAL_SIZE_T prevsize;
int             nextinuse;

atomic_store_relaxed (&amp;av-&gt;have_fastchunks, false);

unsorted_bin = unsorted_chunks(av);

/*
Remove each chunk from fast bin and consolidate it, placing it
then in unsorted bin. Among other reasons for doing this,
placing in unsorted bin avoids needing to calculate actual bins
until malloc is sure that chunks aren't immediately going to be
reused anyway.
*/

maxfb = &amp;fastbin (av, NFASTBINS - 1);
fb = &amp;fastbin (av, 0);
do {
p = atomic_exchange_acquire (fb, NULL);
if (p != 0) {
do {
{
if (__glibc_unlikely (misaligned_chunk (p)))
malloc_printerr ("malloc_consolidate(): "
"unaligned fastbin chunk detected");

unsigned int idx = fastbin_index (chunksize (p));
if ((&amp;fastbin (av, idx)) != fb)
malloc_printerr ("malloc_consolidate(): invalid chunk size");
}

check_inuse_chunk(av, p);
nextp = REVEAL_PTR (p-&gt;fd);

/* Slightly streamlined version of consolidation code in free() */
size = chunksize (p);
nextchunk = chunk_at_offset(p, size);
nextsize = chunksize(nextchunk);

if (!prev_inuse(p)) {
prevsize = prev_size (p);
size += prevsize;
p = chunk_at_offset(p, -((long) prevsize));
if (__glibc_unlikely (chunksize(p) != prevsize))
malloc_printerr ("corrupted size vs. prev_size in fastbins");
unlink_chunk (av, p);
}

if (nextchunk != av-&gt;top) {
nextinuse = inuse_bit_at_offset(nextchunk, nextsize);

if (!nextinuse) {
size += nextsize;
unlink_chunk (av, nextchunk);
} else
clear_inuse_bit_at_offset(nextchunk, 0);

first_unsorted = unsorted_bin-&gt;fd;
unsorted_bin-&gt;fd = p;
first_unsorted-&gt;bk = p;

if (!in_smallbin_range (size)) {
p-&gt;fd_nextsize = NULL;
p-&gt;bk_nextsize = NULL;
}

set_head(p, size | PREV_INUSE);
p-&gt;bk = unsorted_bin;
p-&gt;fd = first_unsorted;
set_foot(p, size);
}

else {
size += nextsize;
set_head(p, size | PREV_INUSE);
av-&gt;top = p;
}

} while ( (p = nextp) != 0);

}
} while (fb++ != maxfb);
}
</code></pre>
</details>
<h3 id="未排序的堆"><a class="header" href="#未排序的堆">未排序的堆</a></h3>
<p>是时候检查未排序的堆以寻找潜在的有效块来使用。</p>
<h4 id="开始"><a class="header" href="#开始">开始</a></h4>
<p>这从一个大的循环开始，该循环将沿着 <code>bk</code> 方向遍历未排序的堆，直到到达末尾（arena 结构），使用 <code>while ((victim = unsorted_chunks (av)-&gt;bk) != unsorted_chunks (av))</code> </p>
<p>此外，每当考虑一个新块时都会进行一些安全检查：</p>
<ul>
<li>如果块大小异常（太小或太大）：<code>malloc(): invalid size (unsorted)</code></li>
<li>如果下一个块大小异常（太小或太大）：<code>malloc(): invalid next size (unsorted)</code></li>
<li>如果下一个块指示的前一个大小与块的大小不同：<code>malloc(): mismatching next-&gt;prev_size (unsorted)</code></li>
<li>如果不是 <code>victim-&gt;bck-&gt;fd == victim</code> 或者不是 <code>victim-&gt;fd == av</code>（arena）：<code>malloc(): unsorted double linked list corrupted</code></li>
<li>由于我们总是检查最后一个，它的 <code>fd</code> 应该始终指向 arena 结构。</li>
<li>如果下一个块没有指示前一个块正在使用：<code>malloc(): invalid next-&gt;prev_inuse (unsorted)</code></li>
</ul>
<details>
<summary><code>_int_malloc</code> 未排序的堆开始</summary>
```c
/*
Process recently freed or remaindered chunks, taking one only if
it is exact fit, or, if this a small request, the chunk is remainder from
the most recent non-exact fit.  Place other traversed chunks in
bins.  Note that this step is the only place in any routine where
chunks are placed in bins.
<p>The outer loop here is needed because we might not realize until
near the end of malloc that we should have consolidated, so must
do so and retry. This happens at most once, and only when we would
otherwise need to expand memory to service a "small" request.
*/</p>
<p>#if USE_TCACHE
INTERNAL_SIZE_T tcache_nb = 0;
size_t tc_idx = csize2tidx (nb);
if (tcache != NULL &amp;&amp; tc_idx &lt; mp_.tcache_bins)
tcache_nb = nb;
int return_cached = 0;</p>
<p>tcache_unsorted_count = 0;
#endif</p>
<p>for (;; )
{
int iters = 0;
while ((victim = unsorted_chunks (av)-&gt;bk) != unsorted_chunks (av))
{
bck = victim-&gt;bk;
size = chunksize (victim);
mchunkptr next = chunk_at_offset (victim, size);</p>
<p>if (__glibc_unlikely (size &lt;= CHUNK_HDR_SZ)
|| __glibc_unlikely (size &gt; av-&gt;system_mem))
malloc_printerr ("malloc(): invalid size (unsorted)");
if (__glibc_unlikely (chunksize_nomask (next) &lt; CHUNK_HDR_SZ)
|| __glibc_unlikely (chunksize_nomask (next) &gt; av-&gt;system_mem))
malloc_printerr ("malloc(): invalid next size (unsorted)");
if (__glibc_unlikely ((prev_size (next) &amp; ~(SIZE_BITS)) != size))
malloc_printerr ("malloc(): mismatching next-&gt;prev_size (unsorted)");
if (__glibc_unlikely (bck-&gt;fd != victim)
|| __glibc_unlikely (victim-&gt;fd != unsorted_chunks (av)))
malloc_printerr ("malloc(): unsorted double linked list corrupted");
if (__glibc_unlikely (prev_inuse (next)))
malloc_printerr ("malloc(): invalid next-&gt;prev_inuse (unsorted)");</p>
<pre><code>&lt;/details&gt;

#### 如果 `in_smallbin_range`

如果块大于请求的大小，则使用它，并将块的其余空间设置为未排序列表，并用它更新 `last_remainder`。

&lt;details&gt;

&lt;summary&gt;&lt;code&gt;_int_malloc&lt;/code&gt; 未排序的 bin &lt;code&gt;in_smallbin_range&lt;/code&gt;&lt;/summary&gt;
```c
// From https://github.com/bminor/glibc/blob/master/malloc/malloc.c#L4090C11-L4124C14

/*
If a small request, try to use last remainder if it is the
only chunk in unsorted bin.  This helps promote locality for
runs of consecutive small requests. This is the only
exception to best-fit, and applies only when there is
no exact fit for a small chunk.
*/

if (in_smallbin_range (nb) &amp;&amp;
bck == unsorted_chunks (av) &amp;&amp;
victim == av-&gt;last_remainder &amp;&amp;
(unsigned long) (size) &gt; (unsigned long) (nb + MINSIZE))
{
/* split and reattach remainder */
remainder_size = size - nb;
remainder = chunk_at_offset (victim, nb);
unsorted_chunks (av)-&gt;bk = unsorted_chunks (av)-&gt;fd = remainder;
av-&gt;last_remainder = remainder;
remainder-&gt;bk = remainder-&gt;fd = unsorted_chunks (av);
if (!in_smallbin_range (remainder_size))
{
remainder-&gt;fd_nextsize = NULL;
remainder-&gt;bk_nextsize = NULL;
}

set_head (victim, nb | PREV_INUSE |
(av != &amp;main_arena ? NON_MAIN_ARENA : 0));
set_head (remainder, remainder_size | PREV_INUSE);
set_foot (remainder, remainder_size);

check_malloced_chunk (av, victim, nb);
void *p = chunk2mem (victim);
alloc_perturb (p, bytes);
return p;
}

</code></pre>
</details>
<p>如果成功，返回块并结束，如果不成功，继续执行函数...</p>
<h4 id="如果大小相等"><a class="header" href="#如果大小相等">如果大小相等</a></h4>
<p>继续从桶中移除块，以防请求的大小正好是块的大小：</p>
<ul>
<li>如果 tcache 没有填满，将其添加到 tcache 中，并继续指示可以使用 tcache 块</li>
<li>如果 tcache 已满，则直接使用它并返回</li>
</ul>
<details>
<summary><code>_int_malloc</code> 未排序桶相等大小</summary>
```c
// From https://github.com/bminor/glibc/blob/master/malloc/malloc.c#L4126C11-L4157C14
<p>/* remove from unsorted list */
unsorted_chunks (av)-&gt;bk = bck;
bck-&gt;fd = unsorted_chunks (av);</p>
<p>/* Take now instead of binning if exact fit */</p>
<p>if (size == nb)
{
set_inuse_bit_at_offset (victim, size);
if (av != &amp;main_arena)
set_non_main_arena (victim);
#if USE_TCACHE
/* Fill cache first, return to user only if cache fills.
We may return one of these chunks later.  */
if (tcache_nb &gt; 0
&amp;&amp; tcache-&gt;counts[tc_idx] &lt; mp_.tcache_count)
{
tcache_put (victim, tc_idx);
return_cached = 1;
continue;
}
else
{
#endif
check_malloced_chunk (av, victim, nb);
void *p = chunk2mem (victim);
alloc_perturb (p, bytes);
return p;
#if USE_TCACHE
}
#endif
}</p>
<pre><code>&lt;/details&gt;

如果块没有被返回或添加到 tcache，继续执行代码...

#### 将块放入一个 bin

根据块的大小将检查过的块存储在小 bin 或大 bin 中（保持大 bin 的正确组织）。

正在执行安全检查，以确保两个大 bin 双向链表没有损坏：

* 如果 `fwd-&gt;bk_nextsize-&gt;fd_nextsize != fwd`: `malloc(): largebin double linked list corrupted (nextsize)`
* 如果 `fwd-&gt;bk-&gt;fd != fwd`: `malloc(): largebin double linked list corrupted (bk)`

&lt;details&gt;

&lt;summary&gt;&lt;code&gt;_int_malloc&lt;/code&gt; 将块放入一个 bin&lt;/summary&gt;
```c
/* place chunk in bin */

if (in_smallbin_range (size))
{
victim_index = smallbin_index (size);
bck = bin_at (av, victim_index);
fwd = bck-&gt;fd;
}
else
{
victim_index = largebin_index (size);
bck = bin_at (av, victim_index);
fwd = bck-&gt;fd;

/* maintain large bins in sorted order */
if (fwd != bck)
{
/* Or with inuse bit to speed comparisons */
size |= PREV_INUSE;
/* if smaller than smallest, bypass loop below */
assert (chunk_main_arena (bck-&gt;bk));
if ((unsigned long) (size)
&lt; (unsigned long) chunksize_nomask (bck-&gt;bk))
{
fwd = bck;
bck = bck-&gt;bk;

victim-&gt;fd_nextsize = fwd-&gt;fd;
victim-&gt;bk_nextsize = fwd-&gt;fd-&gt;bk_nextsize;
fwd-&gt;fd-&gt;bk_nextsize = victim-&gt;bk_nextsize-&gt;fd_nextsize = victim;
}
else
{
assert (chunk_main_arena (fwd));
while ((unsigned long) size &lt; chunksize_nomask (fwd))
{
fwd = fwd-&gt;fd_nextsize;
assert (chunk_main_arena (fwd));
}

if ((unsigned long) size
== (unsigned long) chunksize_nomask (fwd))
/* Always insert in the second position.  */
fwd = fwd-&gt;fd;
else
{
victim-&gt;fd_nextsize = fwd;
victim-&gt;bk_nextsize = fwd-&gt;bk_nextsize;
if (__glibc_unlikely (fwd-&gt;bk_nextsize-&gt;fd_nextsize != fwd))
malloc_printerr ("malloc(): largebin double linked list corrupted (nextsize)");
fwd-&gt;bk_nextsize = victim;
victim-&gt;bk_nextsize-&gt;fd_nextsize = victim;
}
bck = fwd-&gt;bk;
if (bck-&gt;fd != fwd)
malloc_printerr ("malloc(): largebin double linked list corrupted (bk)");
}
}
else
victim-&gt;fd_nextsize = victim-&gt;bk_nextsize = victim;
}

mark_bin (av, victim_index);
victim-&gt;bk = bck;
victim-&gt;fd = fwd;
fwd-&gt;bk = victim;
bck-&gt;fd = victim;
</code></pre>
</details>
<h4 id="_int_malloc-限制"><a class="header" href="#_int_malloc-限制"><code>_int_malloc</code> 限制</a></h4>
<p>在这一点上，如果某个块存储在 tcache 中并且可以使用且达到了限制，就<strong>返回一个 tcache 块</strong>。</p>
<p>此外，如果达到了 <strong>MAX_ITERS</strong>，则从循环中退出并以不同的方式获取一个块（顶块）。</p>
<p>如果设置了 <code>return_cached</code>，则只需从 tcache 返回一个块以避免更大的搜索。</p>
<details>
<summary><code>_int_malloc</code> 限制</summary>
```c
// From https://github.com/bminor/glibc/blob/master/malloc/malloc.c#L4227C1-L4250C7
<p>#if USE_TCACHE
/* If we've processed as many chunks as we're allowed while
filling the cache, return one of the cached ones.  */
++tcache_unsorted_count;
if (return_cached
&amp;&amp; mp_.tcache_unsorted_limit &gt; 0
&amp;&amp; tcache_unsorted_count &gt; mp_.tcache_unsorted_limit)
{
return tcache_get (tc_idx);
}
#endif</p>
<p>#define MAX_ITERS       10000
if (++iters &gt;= MAX_ITERS)
break;
}</p>
<p>#if USE_TCACHE
/* If all the small chunks we found ended up cached, return one now.  */
if (return_cached)
{
return tcache_get (tc_idx);
}
#endif</p>
<pre><code>&lt;/details&gt;

如果未达到限制，请继续执行代码...

### 大型桶（按索引）

如果请求较大（不在小桶中），并且我们尚未返回任何块，请获取所请求大小在**大型桶**中的**索引**，检查是否**不为空**，或者如果**此桶中最大的块大于**所请求的大小，在这种情况下找到**可以用于所请求大小的最小块**。

如果最终使用的块的剩余空间可以成为一个新块，则将其添加到未排序桶中，并更新last_reminder。

在将剩余部分添加到未排序桶时会进行安全检查：

* `bck-&gt;fd-&gt; bk != bck`: `malloc(): 损坏的未排序块`

&lt;details&gt;

&lt;summary&gt;&lt;code&gt;_int_malloc&lt;/code&gt; 大型桶（按索引）&lt;/summary&gt;
```c
// From https://github.com/bminor/glibc/blob/master/malloc/malloc.c#L4252C7-L4317C10

/*
If a large request, scan through the chunks of current bin in
sorted order to find smallest that fits.  Use the skip list for this.
*/

if (!in_smallbin_range (nb))
{
bin = bin_at (av, idx);

/* skip scan if empty or largest chunk is too small */
if ((victim = first (bin)) != bin
&amp;&amp; (unsigned long) chunksize_nomask (victim)
&gt;= (unsigned long) (nb))
{
victim = victim-&gt;bk_nextsize;
while (((unsigned long) (size = chunksize (victim)) &lt;
(unsigned long) (nb)))
victim = victim-&gt;bk_nextsize;

/* Avoid removing the first entry for a size so that the skip
list does not have to be rerouted.  */
if (victim != last (bin)
&amp;&amp; chunksize_nomask (victim)
== chunksize_nomask (victim-&gt;fd))
victim = victim-&gt;fd;

remainder_size = size - nb;
unlink_chunk (av, victim);

/* Exhaust */
if (remainder_size &lt; MINSIZE)
{
set_inuse_bit_at_offset (victim, size);
if (av != &amp;main_arena)
set_non_main_arena (victim);
}
/* Split */
else
{
remainder = chunk_at_offset (victim, nb);
/* We cannot assume the unsorted list is empty and therefore
have to perform a complete insert here.  */
bck = unsorted_chunks (av);
fwd = bck-&gt;fd;
if (__glibc_unlikely (fwd-&gt;bk != bck))
malloc_printerr ("malloc(): corrupted unsorted chunks");
last_re-&gt;bk = bck;
remainder-&gt;fd = fwd;
bck-&gt;fd = remainder;
fwd-&gt;bk = remainder;
if (!in_smallbin_range (remainder_size))
{
remainder-&gt;fd_nextsize = NULL;
remainder-&gt;bk_nextsize = NULL;
}
set_head (victim, nb | PREV_INUSE |
(av != &amp;main_arena ? NON_MAIN_ARENA : 0));
set_head (remainder, remainder_size | PREV_INUSE);
set_foot (remainder, remainder_size);
}
check_malloced_chunk (av, victim, nb);
void *p = chunk2mem (victim);
alloc_perturb (p, bytes);
return p;
}
}
</code></pre>
</details>
<p>如果没有找到合适的块，继续</p>
<h3 id="大块下一个更大"><a class="header" href="#大块下一个更大">大块（下一个更大）</a></h3>
<p>如果在确切的大块中没有任何可以使用的块，开始循环遍历所有下一个大块（从立即更大的开始），直到找到一个（如果有的话）。</p>
<p>分割块的剩余部分被添加到未排序的块中，last_reminder 被更新，并且执行相同的安全检查：</p>
<ul>
<li><code>bck-&gt;fd-&gt; bk != bck</code>: <code>malloc(): corrupted unsorted chunks2</code></li>
</ul>
<details>
<summary><code>_int_malloc</code> 大块（下一个更大）</summary>
```c
// From https://github.com/bminor/glibc/blob/master/malloc/malloc.c#L4319C7-L4425C10
<p>/*
Search for a chunk by scanning bins, starting with next largest
bin. This search is strictly by best-fit; i.e., the smallest
(with ties going to approximately the least recently used) chunk
that fits is selected.</p>
<p>The bitmap avoids needing to check that most blocks are nonempty.
The particular case of skipping all bins during warm-up phases
when no chunks have been returned yet is faster than it might look.
*/</p>
<p>++idx;
bin = bin_at (av, idx);
block = idx2block (idx);
map = av-&gt;binmap[block];
bit = idx2bit (idx);</p>
<p>for (;; )
{
/* Skip rest of block if there are no more set bits in this block.  <em>/
if (bit &gt; map || bit == 0)
{
do
{
if (++block &gt;= BINMAPSIZE) /</em> out of bins */
goto use_top;
}
while ((map = av-&gt;binmap[block]) == 0);</p>
<p>bin = bin_at (av, (block &lt;&lt; BINMAPSHIFT));
bit = 1;
}</p>
<p>/* Advance to bin with set bit. There must be one. */
while ((bit &amp; map) == 0)
{
bin = next_bin (bin);
bit &lt;&lt;= 1;
assert (bit != 0);
}</p>
<p>/* Inspect the bin. It is likely to be non-empty */
victim = last (bin);</p>
<p>/*  If a false alarm (empty bin), clear the bit. <em>/
if (victim == bin)
{
av-&gt;binmap[block] = map &amp;= ~bit; /</em> Write through */
bin = next_bin (bin);
bit &lt;&lt;= 1;
}</p>
<p>else
{
size = chunksize (victim);</p>
<p>/*  We know the first chunk in this bin is big enough to use. */
assert ((unsigned long) (size) &gt;= (unsigned long) (nb));</p>
<p>remainder_size = size - nb;</p>
<p>/* unlink */
unlink_chunk (av, victim);</p>
<p>/* Exhaust */
if (remainder_size &lt; MINSIZE)
{
set_inuse_bit_at_offset (victim, size);
if (av != &amp;main_arena)
set_non_main_arena (victim);
}</p>
<p>/* Split */
else
{
remainder = chunk_at_offset (victim, nb);</p>
<p>/* We cannot assume the unsorted list is empty and therefore
have to perform a complete insert here.  */
bck = unsorted_chunks (av);
fwd = bck-&gt;fd;
if (__glibc_unlikely (fwd-&gt;bk != bck))
malloc_printerr ("malloc(): corrupted unsorted chunks 2");
remainder-&gt;bk = bck;
remainder-&gt;fd = fwd;
bck-&gt;fd = remainder;
fwd-&gt;bk = remainder;</p>
<p>/* advertise as last remainder */
if (in_smallbin_range (nb))
av-&gt;last_remainder = remainder;
if (!in_smallbin_range (remainder_size))
{
remainder-&gt;fd_nextsize = NULL;
remainder-&gt;bk_nextsize = NULL;
}
set_head (victim, nb | PREV_INUSE |
(av != &amp;main_arena ? NON_MAIN_ARENA : 0));
set_head (remainder, remainder_size | PREV_INUSE);
set_foot (remainder, remainder_size);
}
check_malloced_chunk (av, victim, nb);
void *p = chunk2mem (victim);
alloc_perturb (p, bytes);
return p;
}
}</p>
<pre><code>&lt;/details&gt;

### 顶部块

此时，是时候从顶部块获取一个新块（如果足够大）。

它首先进行安全检查，确保块的大小没有过大（已损坏）：

* `chunksize(av-&gt;top) &gt; av-&gt;system_mem`: `malloc(): corrupted top size`

然后，如果顶部块空间足够大，将其用于创建请求大小的块。\
如果不够大，如果有快速块，则合并它们并重试。\
最后，如果空间仍然不足，使用 `sysmalloc` 分配足够的大小。

&lt;details&gt;

&lt;summary&gt;&lt;code&gt;_int_malloc&lt;/code&gt; 顶部块&lt;/summary&gt;
```c
use_top:
/*
If large enough, split off the chunk bordering the end of memory
(held in av-&gt;top). Note that this is in accord with the best-fit
search rule.  In effect, av-&gt;top is treated as larger (and thus
less well fitting) than any other available chunk since it can
be extended to be as large as necessary (up to system
limitations).

We require that av-&gt;top always exists (i.e., has size &gt;=
MINSIZE) after initialization, so if it would otherwise be
exhausted by current request, it is replenished. (The main
reason for ensuring it exists is that we may need MINSIZE space
to put in fenceposts in sysmalloc.)
*/

victim = av-&gt;top;
size = chunksize (victim);

if (__glibc_unlikely (size &gt; av-&gt;system_mem))
malloc_printerr ("malloc(): corrupted top size");

if ((unsigned long) (size) &gt;= (unsigned long) (nb + MINSIZE))
{
remainder_size = size - nb;
remainder = chunk_at_offset (victim, nb);
av-&gt;top = remainder;
set_head (victim, nb | PREV_INUSE |
(av != &amp;main_arena ? NON_MAIN_ARENA : 0));
set_head (remainder, remainder_size | PREV_INUSE);

check_malloced_chunk (av, victim, nb);
void *p = chunk2mem (victim);
alloc_perturb (p, bytes);
return p;
}

/* When we are using atomic ops to free fast chunks we can get
here for all block sizes.  */
else if (atomic_load_relaxed (&amp;av-&gt;have_fastchunks))
{
malloc_consolidate (av);
/* restore original bin index */
if (in_smallbin_range (nb))
idx = smallbin_index (nb);
else
idx = largebin_index (nb);
}

/*
Otherwise, relay to handle system-dependent cases
*/
else
{
void *p = sysmalloc (nb, av);
if (p != NULL)
alloc_perturb (p, bytes);
return p;
}
}
}

</code></pre>
</details>
<h2 id="sysmalloc"><a class="header" href="#sysmalloc">sysmalloc</a></h2>
<h3 id="sysmalloc-开始"><a class="header" href="#sysmalloc-开始">sysmalloc 开始</a></h3>
<p>如果 arena 为 null 或请求的大小太大（并且还有允许的 mmaps），则使用 <code>sysmalloc_mmap</code> 分配空间并返回。</p>
<details>
<summary>sysmalloc 开始</summary>
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L2531
<p>/*
sysmalloc handles malloc cases requiring more memory from the system.
On entry, it is assumed that av-&gt;top does not have enough
space to service request for nb bytes, thus requiring that av-&gt;top
be extended or replaced.
*/</p>
<p>static void *
sysmalloc (INTERNAL_SIZE_T nb, mstate av)
{
mchunkptr old_top;              /* incoming value of av-&gt;top <em>/
INTERNAL_SIZE_T old_size;       /</em> its size */
char <em>old_end;                  /</em> its end address */</p>
<p>long size;                      /* arg to first MORECORE or mmap call */
char <em>brk;                      /</em> return value from MORECORE */</p>
<p>long correction;                /* arg to 2nd MORECORE call */
char <em>snd_brk;                  /</em> 2nd return val */</p>
<p>INTERNAL_SIZE_T front_misalign; /* unusable bytes at front of new space <em>/
INTERNAL_SIZE_T end_misalign;   /</em> partial page left at end of new space */
char <em>aligned_brk;              /</em> aligned offset into brk */</p>
<p>mchunkptr p;                    /* the allocated/returned chunk <em>/
mchunkptr remainder;            /</em> remainder from allocation <em>/
unsigned long remainder_size;   /</em> its size */</p>
<p>size_t pagesize = GLRO (dl_pagesize);
bool tried_mmap = false;</p>
<p>/*
If have mmap, and the request size meets the mmap threshold, and
the system supports mmap, and there are few enough currently
allocated mmapped regions, try to directly map this request
rather than expanding top.
*/</p>
<p>if (av == NULL
|| ((unsigned long) (nb) &gt;= (unsigned long) (mp_.mmap_threshold)
&amp;&amp; (mp_.n_mmaps &lt; mp_.n_mmaps_max)))
{
char <em>mm;
if (mp_.hp_pagesize &gt; 0 &amp;&amp; nb &gt;= mp_.hp_pagesize)
{
/</em> There is no need to issue the THP madvise call if Huge Pages are
used directly.  */
mm = sysmalloc_mmap (nb, mp_.hp_pagesize, mp_.hp_flags, av);
if (mm != MAP_FAILED)
return mm;
}
mm = sysmalloc_mmap (nb, pagesize, 0, av);
if (mm != MAP_FAILED)
return mm;
tried_mmap = true;
}</p>
<p>/* There are no usable arenas and mmap also failed.  */
if (av == NULL)
return 0;</p>
<pre><code>&lt;/details&gt;

### sysmalloc 检查

它首先获取旧的 top chunk 信息，并检查以下条件是否为真：

* 旧的堆大小为 0（新堆）
* 前一个堆的大小大于 MINSIZE 且旧的 Top 正在使用中
* 堆与页面大小对齐（0x1000，因此低 12 位需要为 0）

然后它还检查：

* 旧的大小没有足够的空间为请求的大小创建一个 chunk

&lt;details&gt;

&lt;summary&gt;sysmalloc 检查&lt;/summary&gt;
```c
/* Record incoming configuration of top */

old_top = av-&gt;top;
old_size = chunksize (old_top);
old_end = (char *) (chunk_at_offset (old_top, old_size));

brk = snd_brk = (char *) (MORECORE_FAILURE);

/*
If not the first time through, we require old_size to be
at least MINSIZE and to have prev_inuse set.
*/

assert ((old_top == initial_top (av) &amp;&amp; old_size == 0) ||
((unsigned long) (old_size) &gt;= MINSIZE &amp;&amp;
prev_inuse (old_top) &amp;&amp;
((unsigned long) old_end &amp; (pagesize - 1)) == 0));

/* Precondition: not enough current space to satisfy nb request */
assert ((unsigned long) (old_size) &lt; (unsigned long) (nb + MINSIZE));
</code></pre>
</details>
<h3 id="sysmalloc-不是主区域"><a class="header" href="#sysmalloc-不是主区域">sysmalloc 不是主区域</a></h3>
<p>它首先会尝试<strong>扩展</strong>之前的堆。如果不可能，则尝试<strong>分配一个新的堆</strong>并更新指针以便能够使用它。<br />
最后，如果这也不行，尝试调用**<code>sysmalloc_mmap</code>**。 </p>
<details>
<summary>sysmalloc 不是主区域</summary>
```c
if (av != &main_arena)
{
heap_info *old_heap, *heap;
size_t old_heap_size;
<p>/* First try to extend the current heap. */
old_heap = heap_for_ptr (old_top);
old_heap_size = old_heap-&gt;size;
if ((long) (MINSIZE + nb - old_size) &gt; 0
&amp;&amp; grow_heap (old_heap, MINSIZE + nb - old_size) == 0)
{
av-&gt;system_mem += old_heap-&gt;size - old_heap_size;
set_head (old_top, (((char *) old_heap + old_heap-&gt;size) - (char *) old_top)
| PREV_INUSE);
}
else if ((heap = new_heap (nb + (MINSIZE + sizeof (<em>heap)), mp_.top_pad)))
{
/</em> Use a newly allocated heap.  <em>/
heap-&gt;ar_ptr = av;
heap-&gt;prev = old_heap;
av-&gt;system_mem += heap-&gt;size;
/</em> Set up the new top.  */
top (av) = chunk_at_offset (heap, sizeof (*heap));
set_head (top (av), (heap-&gt;size - sizeof (*heap)) | PREV_INUSE);</p>
<p>/* Setup fencepost and free the old top chunk with a multiple of
MALLOC_ALIGNMENT in size. <em>/
/</em> The fencepost takes at least MINSIZE bytes, because it might
become the top chunk again later.  Note that a footer is set
up, too, although the chunk is marked in use. <em>/
old_size = (old_size - MINSIZE) &amp; ~MALLOC_ALIGN_MASK;
set_head (chunk_at_offset (old_top, old_size + CHUNK_HDR_SZ),
0 | PREV_INUSE);
if (old_size &gt;= MINSIZE)
{
set_head (chunk_at_offset (old_top, old_size),
CHUNK_HDR_SZ | PREV_INUSE);
set_foot (chunk_at_offset (old_top, old_size), CHUNK_HDR_SZ);
set_head (old_top, old_size | PREV_INUSE | NON_MAIN_ARENA);
_int_free (av, old_top, 1);
}
else
{
set_head (old_top, (old_size + CHUNK_HDR_SZ) | PREV_INUSE);
set_foot (old_top, (old_size + CHUNK_HDR_SZ));
}
}
else if (!tried_mmap)
{
/</em> We can at least try to use to mmap memory.  If new_heap fails
it is unlikely that trying to allocate huge pages will
succeed.  */
char *mm = sysmalloc_mmap (nb, pagesize, 0, av);
if (mm != MAP_FAILED)
return mm;
}
}</p>
<pre><code>&lt;/details&gt;

### sysmalloc 主区域

它开始计算所需的内存量。它将首先请求连续的内存，因此在这种情况下可以使用未使用的旧内存。同时还会执行一些对齐操作。

&lt;details&gt;

&lt;summary&gt;sysmalloc 主区域&lt;/summary&gt;
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L2665C1-L2713C10

else     /* av == main_arena */


{ /* Request enough space for nb + pad + overhead */
size = nb + mp_.top_pad + MINSIZE;

/*
If contiguous, we can subtract out existing space that we hope to
combine with new space. We add it back later only if
we don't actually get contiguous space.
*/

if (contiguous (av))
size -= old_size;

/*
Round to a multiple of page size or huge page size.
If MORECORE is not contiguous, this ensures that we only call it
with whole-page arguments.  And if MORECORE is contiguous and
this is not first time through, this preserves page-alignment of
previous calls. Otherwise, we correct to page-align below.
*/

#ifdef MADV_HUGEPAGE
/* Defined in brk.c.  */
extern void *__curbrk;
if (__glibc_unlikely (mp_.thp_pagesize != 0))
{
uintptr_t top = ALIGN_UP ((uintptr_t) __curbrk + size,
mp_.thp_pagesize);
size = top - (uintptr_t) __curbrk;
}
else
#endif
size = ALIGN_UP (size, GLRO(dl_pagesize));

/*
Don't try to call MORECORE if argument is so big as to appear
negative. Note that since mmap takes size_t arg, it may succeed
below even if we cannot call MORECORE.
*/

if (size &gt; 0)
{
brk = (char *) (MORECORE (size));
if (brk != (char *) (MORECORE_FAILURE))
madvise_thp (brk, size);
LIBC_PROBE (memory_sbrk_more, 2, brk, size);
}
</code></pre>
</details>
<h3 id="sysmalloc-主区域之前的错误-1"><a class="header" href="#sysmalloc-主区域之前的错误-1">sysmalloc 主区域之前的错误 1</a></h3>
<p>如果之前返回了 <code>MORECORE_FAILURE</code>，请再次尝试使用 <code>sysmalloc_mmap_fallback</code> 分配内存</p>
<details>
<summary><code>sysmalloc</code> 主区域之前的错误 1</summary>
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L2715C7-L2740C10
<p>if (brk == (char <em>) (MORECORE_FAILURE))
{
/</em>
If have mmap, try using it as a backup when MORECORE fails or
cannot be used. This is worth doing on systems that have "holes" in
address space, so sbrk cannot extend to give contiguous space, but
space is available elsewhere.  Note that we ignore mmap max count
and threshold limits, since the space will not be used as a
segregated mmap region.
*/</p>
<p>char <em>mbrk = MAP_FAILED;
if (mp_.hp_pagesize &gt; 0)
mbrk = sysmalloc_mmap_fallback (&amp;size, nb, old_size,
mp_.hp_pagesize, mp_.hp_pagesize,
mp_.hp_flags, av);
if (mbrk == MAP_FAILED)
mbrk = sysmalloc_mmap_fallback (&amp;size, nb, old_size, MMAP_AS_MORECORE_SIZE,
pagesize, 0, av);
if (mbrk != MAP_FAILED)
{
/</em> We do not need, and cannot use, another sbrk call to find end */
brk = mbrk;
snd_brk = brk + size;
}
}</p>
<pre><code>&lt;/details&gt;

### sysmalloc 主区域继续

如果之前没有返回 `MORECORE_FAILURE`，如果成功则创建一些对齐：

&lt;details&gt;

&lt;summary&gt;sysmalloc 主区域之前的错误 2&lt;/summary&gt;
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L2742

if (brk != (char *) (MORECORE_FAILURE))
{
if (mp_.sbrk_base == 0)
mp_.sbrk_base = brk;
av-&gt;system_mem += size;

/*
If MORECORE extends previous space, we can likewise extend top size.
*/

if (brk == old_end &amp;&amp; snd_brk == (char *) (MORECORE_FAILURE))
set_head (old_top, (size + old_size) | PREV_INUSE);

else if (contiguous (av) &amp;&amp; old_size &amp;&amp; brk &lt; old_end)
/* Oops!  Someone else killed our space..  Can't touch anything.  */
malloc_printerr ("break adjusted to free malloc space");

/*
Otherwise, make adjustments:

* If the first time through or noncontiguous, we need to call sbrk
just to find out where the end of memory lies.

* We need to ensure that all returned chunks from malloc will meet
MALLOC_ALIGNMENT

* If there was an intervening foreign sbrk, we need to adjust sbrk
request size to account for fact that we will not be able to
combine new space with existing space in old_top.

* Almost all systems internally allocate whole pages at a time, in
which case we might as well use the whole last page of request.
So we allocate enough more memory to hit a page boundary now,
which in turn causes future contiguous calls to page-align.
*/

else
{
front_misalign = 0;
end_misalign = 0;
correction = 0;
aligned_brk = brk;

/* handle contiguous cases */
if (contiguous (av))
{
/* Count foreign sbrk as system_mem.  */
if (old_size)
av-&gt;system_mem += brk - old_end;

/* Guarantee alignment of first new chunk made from this space */

front_misalign = (INTERNAL_SIZE_T) chunk2mem (brk) &amp; MALLOC_ALIGN_MASK;
if (front_misalign &gt; 0)
{
/*
Skip over some bytes to arrive at an aligned position.
We don't need to specially mark these wasted front bytes.
They will never be accessed anyway because
prev_inuse of av-&gt;top (and any chunk created from its start)
is always true after initialization.
*/

correction = MALLOC_ALIGNMENT - front_misalign;
aligned_brk += correction;
}

/*
If this isn't adjacent to existing space, then we will not
be able to merge with old_top space, so must add to 2nd request.
*/

correction += old_size;

/* Extend the end address to hit a page boundary */
end_misalign = (INTERNAL_SIZE_T) (brk + size + correction);
correction += (ALIGN_UP (end_misalign, pagesize)) - end_misalign;

assert (correction &gt;= 0);
snd_brk = (char *) (MORECORE (correction));

/*
If can't allocate correction, try to at least find out current
brk.  It might be enough to proceed without failing.

Note that if second sbrk did NOT fail, we assume that space
is contiguous with first sbrk. This is a safe assumption unless
program is multithreaded but doesn't use locks and a foreign sbrk
occurred between our first and second calls.
*/

if (snd_brk == (char *) (MORECORE_FAILURE))
{
correction = 0;
snd_brk = (char *) (MORECORE (0));
}
else
madvise_thp (snd_brk, correction);
}

/* handle non-contiguous cases */
else
{
if (MALLOC_ALIGNMENT == CHUNK_HDR_SZ)
/* MORECORE/mmap must correctly align */
assert (((unsigned long) chunk2mem (brk) &amp; MALLOC_ALIGN_MASK) == 0);
else
{
front_misalign = (INTERNAL_SIZE_T) chunk2mem (brk) &amp; MALLOC_ALIGN_MASK;
if (front_misalign &gt; 0)
{
/*
Skip over some bytes to arrive at an aligned position.
We don't need to specially mark these wasted front bytes.
They will never be accessed anyway because
prev_inuse of av-&gt;top (and any chunk created from its start)
is always true after initialization.
*/

aligned_brk += MALLOC_ALIGNMENT - front_misalign;
}
}

/* Find out current end of memory */
if (snd_brk == (char *) (MORECORE_FAILURE))
{
snd_brk = (char *) (MORECORE (0));
}
}

/* Adjust top based on results of second sbrk */
if (snd_brk != (char *) (MORECORE_FAILURE))
{
av-&gt;top = (mchunkptr) aligned_brk;
set_head (av-&gt;top, (snd_brk - aligned_brk + correction) | PREV_INUSE);
av-&gt;system_mem += correction;

/*
If not the first time through, we either have a
gap due to foreign sbrk or a non-contiguous region.  Insert a
double fencepost at old_top to prevent consolidation with space
we don't own. These fenceposts are artificial chunks that are
marked as inuse and are in any case too small to use.  We need
two to make sizes and alignments work out.
*/

if (old_size != 0)
{
/*
Shrink old_top to insert fenceposts, keeping size a
multiple of MALLOC_ALIGNMENT. We know there is at least
enough space in old_top to do this.
*/
old_size = (old_size - 2 * CHUNK_HDR_SZ) &amp; ~MALLOC_ALIGN_MASK;
set_head (old_top, old_size | PREV_INUSE);

/*
Note that the following assignments completely overwrite
old_top when old_size was previously MINSIZE.  This is
intentional. We need the fencepost, even if old_top otherwise gets
lost.
*/
set_head (chunk_at_offset (old_top, old_size),
CHUNK_HDR_SZ | PREV_INUSE);
set_head (chunk_at_offset (old_top,
old_size + CHUNK_HDR_SZ),
CHUNK_HDR_SZ | PREV_INUSE);

/* If possible, release the rest. */
if (old_size &gt;= MINSIZE)
{
_int_free (av, old_top, 1);
}
}
}
}
}
} /* if (av !=  &amp;main_arena) */
</code></pre>
</details>
<h3 id="sysmalloc-finale"><a class="header" href="#sysmalloc-finale">sysmalloc finale</a></h3>
<p>完成分配，更新区域信息</p>
<pre><code class="language-c">// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L2921C3-L2943C12

if ((unsigned long) av-&gt;system_mem &gt; (unsigned long) (av-&gt;max_system_mem))
av-&gt;max_system_mem = av-&gt;system_mem;
check_malloc_state (av);

/* finally, do the allocation */
p = av-&gt;top;
size = chunksize (p);

/* check that one of the above allocation paths succeeded */
if ((unsigned long) (size) &gt;= (unsigned long) (nb + MINSIZE))
{
remainder_size = size - nb;
remainder = chunk_at_offset (p, nb);
av-&gt;top = remainder;
set_head (p, nb | PREV_INUSE | (av != &amp;main_arena ? NON_MAIN_ARENA : 0));
set_head (remainder, remainder_size | PREV_INUSE);
check_malloced_chunk (av, p, nb);
return chunk2mem (p);
}

/* catch all failure paths */
__set_errno (ENOMEM);
return 0;
</code></pre>
<h2 id="sysmalloc_mmap"><a class="header" href="#sysmalloc_mmap">sysmalloc_mmap</a></h2>
<details>
<summary>sysmalloc_mmap 代码</summary>
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L2392C1-L2481C2
<p>static void *
sysmalloc_mmap (INTERNAL_SIZE_T nb, size_t pagesize, int extra_flags, mstate av)
{
long int size;</p>
<p>/*
Round up size to nearest page.  For mmapped chunks, the overhead is one
SIZE_SZ unit larger than for normal chunks, because there is no
following chunk whose prev_size field could be used.</p>
<p>See the front_misalign handling below, for glibc there is no need for
further alignments unless we have have high alignment.
*/
if (MALLOC_ALIGNMENT == CHUNK_HDR_SZ)
size = ALIGN_UP (nb + SIZE_SZ, pagesize);
else
size = ALIGN_UP (nb + SIZE_SZ + MALLOC_ALIGN_MASK, pagesize);</p>
<p>/* Don't try if size wraps around 0.  */
if ((unsigned long) (size) &lt;= (unsigned long) (nb))
return MAP_FAILED;</p>
<p>char *mm = (char *) MMAP (0, size,
mtag_mmap_flags | PROT_READ | PROT_WRITE,
extra_flags);
if (mm == MAP_FAILED)
return mm;</p>
<p>#ifdef MAP_HUGETLB
if (!(extra_flags &amp; MAP_HUGETLB))
madvise_thp (mm, size);
#endif</p>
<p>__set_vma_name (mm, size, " glibc: malloc");</p>
<p>/*
The offset to the start of the mmapped region is stored in the prev_size
field of the chunk.  This allows us to adjust returned start address to
meet alignment requirements here and in memalign(), and still be able to
compute proper address argument for later munmap in free() and realloc().
*/</p>
<p>INTERNAL_SIZE_T front_misalign; /* unusable bytes at front of new space */</p>
<p>if (MALLOC_ALIGNMENT == CHUNK_HDR_SZ)
{
/* For glibc, chunk2mem increases the address by CHUNK_HDR_SZ and
MALLOC_ALIGN_MASK is CHUNK_HDR_SZ-1.  Each mmap'ed area is page
aligned and therefore definitely MALLOC_ALIGN_MASK-aligned.  */
assert (((INTERNAL_SIZE_T) chunk2mem (mm) &amp; MALLOC_ALIGN_MASK) == 0);
front_misalign = 0;
}
else
front_misalign = (INTERNAL_SIZE_T) chunk2mem (mm) &amp; MALLOC_ALIGN_MASK;</p>
<p>mchunkptr p;                    /* the allocated/returned chunk */</p>
<p>if (front_misalign &gt; 0)
{
ptrdiff_t correction = MALLOC_ALIGNMENT - front_misalign;
p = (mchunkptr) (mm + correction);
set_prev_size (p, correction);
set_head (p, (size - correction) | IS_MMAPPED);
}
else
{
p = (mchunkptr) mm;
set_prev_size (p, 0);
set_head (p, size | IS_MMAPPED);
}</p>
<p>/* update statistics */
int new = atomic_fetch_add_relaxed (&amp;mp_.n_mmaps, 1) + 1;
atomic_max (&amp;mp_.max_n_mmaps, new);</p>
<p>unsigned long sum;
sum = atomic_fetch_add_relaxed (&amp;mp_.mmapped_mem, size) + size;
atomic_max (&amp;mp_.max_mmapped_mem, sum);</p>
<p>check_chunk (av, p);</p>
<p>return chunk2mem (p);
}</p>
<pre><code>&lt;/details&gt;

{% hint style="success" %}
学习与实践 AWS 黑客技术：&lt;img src="/.gitbook/assets/arte.png" alt="" data-size="line"&gt;[**HackTricks 培训 AWS 红队专家 (ARTE)**](https://training.hacktricks.xyz/courses/arte)&lt;img src="/.gitbook/assets/arte.png" alt="" data-size="line"&gt;\
学习与实践 GCP 黑客技术：&lt;img src="/.gitbook/assets/grte.png" alt="" data-size="line"&gt;[**HackTricks 培训 GCP 红队专家 (GRTE)**&lt;img src="/.gitbook/assets/grte.png" alt="" data-size="line"&gt;](https://training.hacktricks.xyz/courses/grte)

&lt;details&gt;

&lt;summary&gt;支持 HackTricks&lt;/summary&gt;

* 查看 [**订阅计划**](https://github.com/sponsors/carlospolop)!
* **加入** 💬 [**Discord 群组**](https://discord.gg/hRep4RUj7f) 或 [**电报群组**](https://t.me/peass) 或 **在** **Twitter** 🐦 **上关注我们** [**@hacktricks\_live**](https://twitter.com/hacktricks\_live)**.**
* **通过向** [**HackTricks**](https://github.com/carlospolop/hacktricks) 和 [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) github 仓库提交 PR 来分享黑客技巧。

&lt;/details&gt;
{% endhint %}
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../../binary-exploitation/libc-heap/heap-memory-functions/free.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../../../binary-exploitation/libc-heap/heap-memory-functions/unlink.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../../binary-exploitation/libc-heap/heap-memory-functions/free.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../../../binary-exploitation/libc-heap/heap-memory-functions/unlink.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../../../elasticlunr.min.js"></script>
        <script src="../../../mark.min.js"></script>
        <script src="../../../searcher.js"></script>

        <script src="../../../clipboard.min.js"></script>
        <script src="../../../highlight.js"></script>
        <script src="../../../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
